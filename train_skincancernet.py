import tensorflow as tf
from tensorflow import keras

import numpy as np
import os, argparse, pathlib
import pandas as pd

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.models import load_model

from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler

from data import Skin_Lesion_Dataset_Balanced


parser = argparse.ArgumentParser(description='SkinCancerNet Training Script')
parser.add_argument('--epochs', default=10, type=int, help='Number of epochs')
parser.add_argument('--lr', default=0.001, type=float, help='Learning rate')
parser.add_argument('--bs', default=8, type=int, help='Batch size')
parser.add_argument('--trainfile', default='train_images.csv', type=str, help='Name of train csv file')
parser.add_argument('--valfile', default='val_images.csv', type=str, help='Name of test csv file')
parser.add_argument('--savepath', default='SkinCancerNet_Output', type=str, help='Name of folder to store training model outputs')
parser.add_argument('--modelname', default='SkinCancerNet', type=str, help='Name of the model')
parser.add_argument('--datadir', default='data', type=str, help='Path to data folder')
parser.add_argument('--malignantpercent', default=0.5, type=float, help='Percentage of malignant samples to upsample to in each batch')

args = parser.parse_args()

# Parameters
learning_rate = args.lr
batch_size = args.bs
num_epochs = args.epochs

# output path
output_path = './' + args.savepath
pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)
print('Output: ' + output_path)
model_name = args.modelname + '-{epoch:03d}.h5'


# Exponential Learning Rate Decay
def lr_schedule_expo(epoch):
    learning_rate_decay_start = 20
    learning_rate_decay_every = 3
    learning_rate_decay_rate = 0.85
    learning_rate = 0.001
    if epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:
        frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every
        decay_factor = learning_rate_decay_rate ** frac
        learning_rate = learning_rate * decay_factor
    else:
        learning_rate = learning_rate
    return learning_rate

# Start a new session
session = keras.backend.get_session()
init = tf.global_variables_initializer()
session.run(init)

# Set up data generators
train_generator = Skin_Lesion_Dataset_Balanced(data_dir=args.datadir,
                                     csv_file=args.trainfile,
                                     is_training=True,
                                     batch_size=batch_size,
                                     malignant_percent=args.malignantpercent)
val_generator = Skin_Lesion_Dataset_Balanced(data_dir=args.datadir,
                                     csv_file=args.valfile,
                                     is_training=False,
                                     batch_size=batch_size,
                                     malignant_percent=args.malignantpercent)

model = ResNet50(include_top=True, weights=None, classes=2)
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=lr_schedule_expo(0)),
              metrics=['accuracy'])

filepath = os.path.join(output_path, model_name)
# Prepare callbacks for model saving and for learning rate adjustment.
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=False)

lr_scheduler = LearningRateScheduler(lr_schedule_expo)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

# Fit the model on the batches generated by datagen.flow().
history = model.fit_generator(generator=train_generator,
                    validation_data=val_generator,
                    epochs=num_epochs, steps_per_epoch=None, verbose=1,
                    workers=1, use_multiprocessing=True, shuffle=True, callbacks=callbacks)

# Save history training logs
pd.DataFrame.from_dict(history.history).to_csv(base_path + 'results.csv',index=False)
